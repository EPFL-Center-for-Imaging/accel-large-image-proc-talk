{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "# 2 - Introduction to `Dask`\n",
    "    \n",
    "<img src=\"imgs/dask_logo.png\" alt=\"dask\" width=\"250\"/>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) What is Dask?\n",
    "\n",
    "\n",
    "[Dask](https://www.dask.org/) is a software framework that allows you to work with large datasets in parallel on your computer or a cluster of computers. \n",
    "\n",
    "- **Chunking**: Chunk data (Arrays, Dataframes, etc.) too big to fit into RAM by breaking it up into smaller chunks. \n",
    "\n",
    "- **Dynamic scheduling**: Divides the computational task and run it on different workers, according to their availability (*perform a task faster*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>   \n",
    "<img src=\"imgs/dask-overview.png\" alt=\"dask-overview\" width=\"900\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create a `dask.array`\n",
    "\n",
    "[Dask Array](https://docs.dask.org/en/stable/array.html) implements a subset of the NumPy ndarray interface using **blocked algorithms** and **chunking** the large arrays into many small arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "x = da.random.random((200_000, 1000), chunks=(50_000, 500))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dask is lazy by default!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(f\"Size of `x`: {getsizeof(x)} bytes\")\n",
    "print(f\"Size of `x.compute()`: {getsizeof(x.compute()) * 1e-9 : 0.2f} Gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Add layers to the computational task graph.\n",
    "\n",
    "Chunking allows us perform computations that are larger than existing memory by using all of our machine cores.\n",
    "\n",
    "The blocked algorithms are coordintated using [Dask Task Graphs](https://docs.dask.org/en/stable/graphs.html). \n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sum = x.sum(1).sum(0)\n",
    "x_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(x_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Because Dask is lazy by default, we have to call the compute method to obtain the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the how Dask scales computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "# One chunk\n",
    "x_short = da.random.random((5_000, 1000), chunks=(5_000, 1000)) \n",
    "# One chunk - 100 times larger\n",
    "x_long = da.random.random((500_000, 1000), chunks=(500_000, 1000)) \n",
    "# 100 chunks - 100 times larger\n",
    "x_long_chunks = da.random.random((500_000, 1000), chunks=(5_000, 1000)) \n",
    "\n",
    "%timeit x_short.sum().compute()\n",
    "%timeit x_long.sum().compute()\n",
    "%timeit x_long_chunks.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `x_long` is array is 100 times larger than `x_short`. when there is no chunking, the computation takes ~100 times longer. However, if we chunk `x_long` into 100 chunks, the computation takes only ~20 times longer.\n",
    "\n",
    "This is because Dask is multi-threading the computation of the different parts of the task graph, allowing to run them in ***parallel***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Create a Dask Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting a *Dask Client* is optional (there is a default scheduler). However, creating it will provide a **dashboard** which is useful to gain insight on the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=4, threads_per_worker=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's look at the Dask dashboard. Click on the printed link above and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = da.random.random((10_000, 3_000, 100), chunks=(1_000, 1_000, 50))\n",
    "y = da.random.random((10_000, 3_000, 100), chunks=(1_000, 1_000, 50))\n",
    "z = (da.arcsin(x) + da.arccos(y)).sum(axis=(1, 2))\n",
    "\n",
    "%time z.sum(0).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist data in memory\n",
    "\n",
    "If you have the available RAM for your dataset then you can persist data in memory.\n",
    "\n",
    "This allows future computations to be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((10_000, 3_000, 100), chunks=(1_000, 1_000, 50))\n",
    "y = da.random.random((10_000, 3_000, 100), chunks=(1_000, 1_000, 50))\n",
    "\n",
    "# persist z\n",
    "z = (da.arcsin(x) + da.arccos(y)).sum(axis=(1, 2)).persist()\n",
    "\n",
    "%time z.sum(0).compute()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
